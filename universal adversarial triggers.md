Notes from Universal Adversarial Triggers for Attacking and Analyzing NLP
https://arxiv.org/pdf/1908.07125


definition from eric wallace: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset


Adversarial attacks are also useful for evaluation, limitaiton identification and facilitate interpretation (eg: analyzing a modelâ€™s sensitivity to local perturbations)

implications: 
- error in sentiment analysis and natural language inference model

unlike typical attacks, they are context-independent