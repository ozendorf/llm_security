## Papers on LLM Security / attacks
* [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/pdf/2306.13213) 
* Many articles, blog posts and repos [here](https://github.com/corca-ai/awesome-llm-security?tab=readme-ov-file)
* TensorFlow threat model [here](https://bughunters.google.com/blog/5160301538967552/tensorflow-threat-model-and-security-guidelines-update)
* To read : theory on red teaming on LLMs 

## Bug bounty, vulnerabilities on AI/LLM
* https://huntr.com/

## Open sources tools 
* [giskard](https://www.giskard.ai/)
* test
* test 
## Market solutions 

## Conferences
*

## Books
* https://nostarch.com/how-ai-works
* The Developer's Playbook for Large Language Model Security: Building Secure AI Applications
