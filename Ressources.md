## Papers on LLM Security / attacks
* [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/pdf/2306.13213) 
* Many articles, blog posts and repos [here](https://github.com/corca-ai/awesome-llm-security?tab=readme-ov-file)
* TensorFlow threat model [here](https://bughunters.google.com/blog/5160301538967552/tensorflow-threat-model-and-security-guidelines-update)
* To read : theory on red teaming on LLMs 


## Standards / Compliance on AI Security 
* [CISA and NCSC's Take on Secure AI Development](https://www.resilientcyber.io/p/cisa-and-ncscs-take-on-secure-ai)
## Bug bounty, vulnerabilities on AI/LLM
* https://huntr.com/


* Jailbreaking 
https://flowgpt.com/

## Open sources tools 
* [giskard](https://www.giskard.ai/)
* http://easyjailbreak.org/
* test 
## Market solutions 

## Conferences
*

## Books
* https://nostarch.com/how-ai-works
* The Developer's Playbook for Large Language Model Security: Building Secure AI Applications

## References for demo
* https://redarena.ai/